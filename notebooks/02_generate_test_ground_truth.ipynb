{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth Generation for Building Segmentation\n",
    "\n",
    "Ансамбль Grounded SAM + YOLOv8 с TTA для сегментации зданий (Inria Aerial Dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/code/dls_project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: Tesla V100-SXM2-16GB, 16.9 GB\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from skimage.morphology import remove_small_holes, remove_small_objects\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForZeroShotObjectDetection, AutoProcessor\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Фиксация seed для воспроизводимости\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}, {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: PRODUCTION\n",
      "Images: /home/user/code/dls_project/data/raw/test/images\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Pipeline configuration.\"\"\"\n",
    "    # Режим\n",
    "    debug: bool = False\n",
    "    debug_samples: int = 10\n",
    "    project_root: Path = field(default_factory=lambda: Path.cwd().parent)\n",
    "    \n",
    "    # Тайлинг — 768 оптимально для баланса контекст/детализация\n",
    "    tile_size: int = 768\n",
    "    overlap: int = 192\n",
    "    \n",
    "    # TTA — multi-scale + flips + rotations\n",
    "    use_flips: bool = True\n",
    "    use_rotations: bool = True  # +90°, -90°, 180°\n",
    "    scales: tuple[float, ...] = (0.75, 0.85, 1.0, 1.15, 1.25)  # Расширенный multi-scale для recall\n",
    "    \n",
    "    # Ансамбль — акцент на recall\n",
    "    grounded_sam_weight: float = 0.50\n",
    "    yolo_weight: float = 0.50\n",
    "    threshold: float = 0.12  # Понижен для увеличения recall\n",
    "    \n",
    "    # Постобработка\n",
    "    min_building_area: int = 50  # Понижен для мелких зданий\n",
    "    max_hole_area: int = 600  # Увеличен для заполнения дыр\n",
    "    morph_kernel: int = 5  # Увеличен для лучшего соединения\n",
    "    use_edge_refinement: bool = True  # Bilateral filter для границ\n",
    "    bilateral_d: int = 9\n",
    "    bilateral_sigma_color: float = 75.0\n",
    "    bilateral_sigma_space: float = 75.0\n",
    "    \n",
    "    # Модели\n",
    "    gdino_model: str = \"IDEA-Research/grounding-dino-base\"\n",
    "    sam2_model: str = \"facebook/sam2.1-hiera-large\"\n",
    "    yolo_model: str = \"keremberke/yolov8m-building-segmentation\"\n",
    "    \n",
    "    @property\n",
    "    def data_dir(self) -> Path:\n",
    "        return self.project_root / \"data\" / \"raw\"\n",
    "    \n",
    "    @property\n",
    "    def train_images(self) -> Path:\n",
    "        return self.data_dir / \"train\" / \"images\"\n",
    "    \n",
    "    @property\n",
    "    def train_gt(self) -> Path:\n",
    "        return self.data_dir / \"train\" / \"gt\"\n",
    "    \n",
    "    @property\n",
    "    def test_images(self) -> Path:\n",
    "        return self.data_dir / \"test\" / \"images\"\n",
    "    \n",
    "    @property\n",
    "    def test_gt(self) -> Path:\n",
    "        return self.project_root / \"data\" / \"processed\" / \"test\" / \"gt\"\n",
    "    \n",
    "    @property\n",
    "    def val_gt(self) -> Path:\n",
    "        return self.data_dir / \"train\" / \"gt_val\"\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "cfg.test_gt.mkdir(parents=True, exist_ok=True)\n",
    "cfg.val_gt.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Mode: {'DEBUG' if cfg.debug else 'PRODUCTION'}\")\n",
    "print(f\"Images: {cfg.train_images if cfg.debug else cfg.test_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundedSAMSegmentor:\n",
    "    \"\"\"Grounding DINO + SAM2.1 для детекции и сегментации зданий.\"\"\"\n",
    "    \n",
    "    # Расширенный промпт для лучшего recall\n",
    "    TEXT_PROMPT = \"building . house . roof . warehouse . shed . garage . barn . residential building . commercial building . structure . facility . construction . apartment . villa . cottage . hut . cabin .\"\n",
    "    BOX_THRESHOLD = 0.08  # Понижен для увеличения recall\n",
    "    TEXT_THRESHOLD = 0.08\n",
    "    \n",
    "    def __init__(self, gdino_model: str, sam_model: str):\n",
    "        self.processor = AutoProcessor.from_pretrained(gdino_model, use_fast=True)\n",
    "        self.gdino = AutoModelForZeroShotObjectDetection.from_pretrained(gdino_model).to(DEVICE)\n",
    "        self.gdino.eval()\n",
    "        # SAM2 требует явного указания device\n",
    "        self.sam = SAM2ImagePredictor.from_pretrained(sam_model, device=DEVICE)\n",
    "        print(\"Loaded: Grounded SAM\")\n",
    "    \n",
    "    def predict(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Return probability map [0, 1].\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        prob_map = np.zeros((h, w), dtype=np.float32)\n",
    "        \n",
    "        # Детекция\n",
    "        inputs = self.processor(images=image, text=self.TEXT_PROMPT, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.gdino(**inputs)\n",
    "        \n",
    "        results = self.processor.post_process_grounded_object_detection(\n",
    "            outputs, inputs.input_ids,\n",
    "            threshold=self.BOX_THRESHOLD,\n",
    "            text_threshold=self.TEXT_THRESHOLD,\n",
    "            target_sizes=[(h, w)]\n",
    "        )[0]\n",
    "        \n",
    "        boxes, scores = results[\"boxes\"].cpu().numpy(), results[\"scores\"].cpu().numpy()\n",
    "        if len(boxes) == 0:\n",
    "            return prob_map\n",
    "        \n",
    "        # Сегментация через SAM с inference_mode для эффективности\n",
    "        with torch.inference_mode():\n",
    "            self.sam.set_image(image)\n",
    "            \n",
    "            # Обработка чанками для экономии памяти\n",
    "            for i in range(0, len(boxes), 8):\n",
    "                chunk_boxes = boxes[i:i+8]\n",
    "                chunk_scores = scores[i:i+8]\n",
    "                \n",
    "                masks, iou_scores, _ = self.sam.predict(box=chunk_boxes, multimask_output=True)\n",
    "                \n",
    "                # SAM возвращает разную размерность для 1 бокса vs нескольких\n",
    "                if len(chunk_boxes) == 1:\n",
    "                    masks = masks[np.newaxis, ...]\n",
    "                    iou_scores = iou_scores[np.newaxis, ...]\n",
    "                \n",
    "                for j, det_score in enumerate(chunk_scores):\n",
    "                    best_idx = iou_scores[j].argmax()\n",
    "                    score = det_score * iou_scores[j, best_idx]\n",
    "                    prob_map = np.maximum(prob_map, masks[j, best_idx].astype(np.float32) * score)\n",
    "        \n",
    "        return prob_map\n",
    "\n",
    "\n",
    "class YOLOSegmentor:\n",
    "    \"\"\"YOLOv8, специализированная на сегментации зданий.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        weights_path = hf_hub_download(repo_id=model_name, filename=\"best.pt\")\n",
    "        self.model = YOLO(weights_path)\n",
    "        self.model.overrides.update({\"conf\": 0.04, \"iou\": 0.25, \"max_det\": 3000})  # Понижен conf для recall\n",
    "        self.model.to(DEVICE)\n",
    "        print(\"Loaded: YOLO Building\")\n",
    "    \n",
    "    def predict(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Return probability map [0, 1].\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        prob_map = np.zeros((h, w), dtype=np.float32)\n",
    "        \n",
    "        for result in self.model(image, verbose=False):\n",
    "            if result.masks is not None:\n",
    "                for mask, conf in zip(result.masks.data.cpu().numpy(), result.boxes.conf.cpu().numpy()):\n",
    "                    mask_resized = cv2.resize(mask, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "                    prob_map = np.maximum(prob_map, mask_resized * conf)\n",
    "        \n",
    "        return prob_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundTruthGenerator:\n",
    "    \"\"\"Генератор GT масок с ансамблем и TTA.\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "        self.models: list[tuple[object, float]] = []\n",
    "    \n",
    "    def load_models(self) -> None:\n",
    "        \"\"\"Загрузка моделей.\"\"\"\n",
    "        print(\"Loading models...\")\n",
    "        self.models = [\n",
    "            (GroundedSAMSegmentor(self.cfg.gdino_model, self.cfg.sam2_model), self.cfg.grounded_sam_weight),\n",
    "            (YOLOSegmentor(self.cfg.yolo_model), self.cfg.yolo_weight),\n",
    "        ]\n",
    "        print(f\"Loaded {len(self.models)} models\")\n",
    "    \n",
    "    def _tile_image(self, image: np.ndarray) -> tuple[list[np.ndarray], list[tuple[int, int]]]:\n",
    "        \"\"\"Разбиение на тайлы с overlap.\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        stride = self.cfg.tile_size - self.cfg.overlap\n",
    "        tiles, positions = [], []\n",
    "        \n",
    "        for y in range(0, h, stride):\n",
    "            for x in range(0, w, stride):\n",
    "                y_end, x_end = min(y + self.cfg.tile_size, h), min(x + self.cfg.tile_size, w)\n",
    "                y_start, x_start = max(0, y_end - self.cfg.tile_size), max(0, x_end - self.cfg.tile_size)\n",
    "                tiles.append(image[y_start:y_end, x_start:x_end])\n",
    "                positions.append((y_start, x_start))\n",
    "        \n",
    "        return tiles, positions\n",
    "    \n",
    "    def _merge_tiles(self, tiles: list[np.ndarray], positions: list[tuple[int, int]], shape: tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Объединение тайлов с weighted blending.\"\"\"\n",
    "        h, w = shape\n",
    "        output = np.zeros((h, w), dtype=np.float32)\n",
    "        weights = np.zeros((h, w), dtype=np.float32)\n",
    "        overlap = self.cfg.overlap\n",
    "        \n",
    "        for tile, (y, x) in zip(tiles, positions):\n",
    "            th, tw = tile.shape[:2]\n",
    "            wy, wx = np.ones(th), np.ones(tw)\n",
    "            \n",
    "            if overlap > 0:\n",
    "                ramp = np.linspace(0, 1, overlap)\n",
    "                if y > 0 and th >= overlap:\n",
    "                    wy[:overlap] = ramp\n",
    "                if x > 0 and tw >= overlap:\n",
    "                    wx[:overlap] = ramp\n",
    "                if y + th < h and th >= overlap:\n",
    "                    wy[-overlap:] = ramp[::-1]\n",
    "                if x + tw < w and tw >= overlap:\n",
    "                    wx[-overlap:] = ramp[::-1]\n",
    "            \n",
    "            tile_weight = np.outer(wy, wx)\n",
    "            output[y:y+th, x:x+tw] += tile * tile_weight\n",
    "            weights[y:y+th, x:x+tw] += tile_weight\n",
    "        \n",
    "        return output / np.maximum(weights, 1e-8)\n",
    "    \n",
    "    def _postprocess(self, prob_map: np.ndarray, image: np.ndarray | None = None) -> np.ndarray:\n",
    "        \"\"\"Постобработка: edge refinement + бинаризация + морфология.\"\"\"\n",
    "        # #region agent log\n",
    "        import json; open('/home/user/code/dls_project/.cursor/debug.log','a').write(json.dumps({\"location\":\"cell7:_postprocess\",\"message\":\"prob_map stats before filter\",\"data\":{\"min\":float(prob_map.min()),\"max\":float(prob_map.max()),\"mean\":float(prob_map.mean()),\"shape\":list(prob_map.shape),\"use_edge_ref\":self.cfg.use_edge_refinement,\"has_image\":image is not None},\"hypothesisId\":\"H2,H5\",\"timestamp\":__import__('time').time()})+'\\n')\n",
    "        # #endregion\n",
    "        # Edge-aware refinement через bilateral filter\n",
    "        if self.cfg.use_edge_refinement and image is not None:\n",
    "            # Bilateral filter сглаживает prob_map с учётом границ изображения\n",
    "            prob_uint8 = (prob_map * 255).astype(np.uint8)\n",
    "            prob_filtered = cv2.bilateralFilter(\n",
    "                prob_uint8, \n",
    "                self.cfg.bilateral_d, \n",
    "                self.cfg.bilateral_sigma_color, \n",
    "                self.cfg.bilateral_sigma_space\n",
    "            )\n",
    "            prob_map = prob_filtered.astype(np.float32) / 255.0\n",
    "            # #region agent log\n",
    "            import json; open('/home/user/code/dls_project/.cursor/debug.log','a').write(json.dumps({\"location\":\"cell7:_postprocess\",\"message\":\"prob_map stats after filter\",\"data\":{\"min\":float(prob_map.min()),\"max\":float(prob_map.max()),\"mean\":float(prob_map.mean())},\"hypothesisId\":\"H5\",\"timestamp\":__import__('time').time()})+'\\n')\n",
    "            # #endregion\n",
    "        \n",
    "        binary = (prob_map >= self.cfg.threshold).astype(np.uint8)\n",
    "        \n",
    "        # Морфология: closing для заполнения, opening для шума\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (self.cfg.morph_kernel, self.cfg.morph_kernel))\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "        \n",
    "        # Удаление мелких объектов и заполнение дыр\n",
    "        binary = remove_small_objects(binary.astype(bool), min_size=self.cfg.min_building_area)\n",
    "        binary = remove_small_holes(binary, area_threshold=self.cfg.max_hole_area)\n",
    "        \n",
    "        return binary.astype(np.uint8) * 255\n",
    "    \n",
    "    def _process_tile(self, tile: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Обработка одного тайла ансамблем.\"\"\"\n",
    "        th, tw = tile.shape[:2]\n",
    "        ensemble = np.zeros((th, tw), dtype=np.float32)\n",
    "        # #region agent log\n",
    "        total_weight = sum(mw for _, mw in self.models)\n",
    "        import json; open('/home/user/code/dls_project/.cursor/debug.log','a').write(json.dumps({\"location\":\"cell7:_process_tile\",\"message\":\"total_weight calc\",\"data\":{\"tile_shape\":[th,tw],\"total_weight\":total_weight,\"n_models\":len(self.models)},\"hypothesisId\":\"H1\",\"timestamp\":__import__('time').time()})+'\\n')\n",
    "        # #endregion\n",
    "        \n",
    "        for model, model_weight in self.models:\n",
    "            pred = model.predict(tile)\n",
    "            ensemble += pred * model_weight\n",
    "        \n",
    "        return ensemble / total_weight\n",
    "    \n",
    "    def _process_at_scale(self, image: np.ndarray, scale: float) -> np.ndarray:\n",
    "        \"\"\"Обработка изображения на заданном масштабе.\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        if scale != 1.0:\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            scaled = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "        else:\n",
    "            scaled = image\n",
    "        \n",
    "        tiles, positions = self._tile_image(scaled)\n",
    "        tile_preds = [self._process_tile(t) for t in tiles]\n",
    "        prob = self._merge_tiles(tile_preds, positions, scaled.shape[:2])\n",
    "        \n",
    "        if scale != 1.0:\n",
    "            prob = cv2.resize(prob, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def _rotate_image(self, image: np.ndarray, angle: int) -> np.ndarray:\n",
    "        \"\"\"Поворот изображения на 90, 180 или 270 градусов.\"\"\"\n",
    "        if angle == 90:\n",
    "            return np.rot90(image, k=1)\n",
    "        elif angle == 180:\n",
    "            return np.rot90(image, k=2)\n",
    "        elif angle == 270:\n",
    "            return np.rot90(image, k=3)\n",
    "        return image\n",
    "    \n",
    "    def _unrotate_prob(self, prob: np.ndarray, angle: int) -> np.ndarray:\n",
    "        \"\"\"Обратный поворот probability map.\"\"\"\n",
    "        if angle == 90:\n",
    "            return np.rot90(prob, k=-1)\n",
    "        elif angle == 180:\n",
    "            return np.rot90(prob, k=-2)\n",
    "        elif angle == 270:\n",
    "            return np.rot90(prob, k=-3)\n",
    "        return prob\n",
    "    \n",
    "    def process_image(self, image_path: Path) -> np.ndarray:\n",
    "        \"\"\"Обработка изображения с multi-scale TTA + flips + rotations.\"\"\"\n",
    "        image = np.array(Image.open(image_path))\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        all_probs = []\n",
    "        \n",
    "        # Rotations: 0, 90, 180, 270 градусов\n",
    "        rotations = [0]\n",
    "        if self.cfg.use_rotations:\n",
    "            rotations.extend([90, 180, 270])\n",
    "        # #region agent log\n",
    "        import json; open('/home/user/code/dls_project/.cursor/debug.log','a').write(json.dumps({\"location\":\"cell7:process_image\",\"message\":\"TTA config\",\"data\":{\"image_shape\":[h,w],\"rotations\":rotations,\"scales\":list(self.cfg.scales),\"use_flips\":self.cfg.use_flips,\"expected_passes\":len(rotations)*len(self.cfg.scales)*(3 if self.cfg.use_flips else 1)},\"hypothesisId\":\"H3,H4\",\"timestamp\":__import__('time').time()})+'\\n')\n",
    "        # #endregion\n",
    "        \n",
    "        for rotation in rotations:\n",
    "            rotated = self._rotate_image(image, rotation).copy()\n",
    "            \n",
    "            # Multi-scale TTA\n",
    "            for scale in self.cfg.scales:\n",
    "                # Базовая обработка\n",
    "                prob = self._process_at_scale(rotated, scale)\n",
    "                prob = self._unrotate_prob(prob, rotation)\n",
    "                # Проверка размера после unrotate (для не-квадратных изображений)\n",
    "                if prob.shape != (h, w):\n",
    "                    prob = cv2.resize(prob, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "                all_probs.append(prob)\n",
    "                \n",
    "                # Flip augmentations\n",
    "                if self.cfg.use_flips:\n",
    "                    # Horizontal flip\n",
    "                    flipped_h = np.fliplr(rotated).copy()\n",
    "                    prob_h = self._process_at_scale(flipped_h, scale)\n",
    "                    prob_h = np.fliplr(prob_h)\n",
    "                    prob_h = self._unrotate_prob(prob_h, rotation)\n",
    "                    if prob_h.shape != (h, w):\n",
    "                        prob_h = cv2.resize(prob_h, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "                    all_probs.append(prob_h)\n",
    "                    \n",
    "                    # Vertical flip\n",
    "                    flipped_v = np.flipud(rotated).copy()\n",
    "                    prob_v = self._process_at_scale(flipped_v, scale)\n",
    "                    prob_v = np.flipud(prob_v)\n",
    "                    prob_v = self._unrotate_prob(prob_v, rotation)\n",
    "                    if prob_v.shape != (h, w):\n",
    "                        prob_v = cv2.resize(prob_v, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "                    all_probs.append(prob_v)\n",
    "        \n",
    "        # Медиана вместо среднего — более робастна к выбросам\n",
    "        # #region agent log\n",
    "        import json; open('/home/user/code/dls_project/.cursor/debug.log','a').write(json.dumps({\"location\":\"cell7:process_image\",\"message\":\"TTA complete\",\"data\":{\"actual_passes\":len(all_probs),\"prob_shapes\":[list(p.shape) for p in all_probs[:3]],\"all_same_shape\":all(p.shape==(h,w) for p in all_probs)},\"hypothesisId\":\"H3,H4\",\"timestamp\":__import__('time').time()})+'\\n')\n",
    "        # #endregion\n",
    "        median_prob = np.median(all_probs, axis=0)\n",
    "        return self._postprocess(median_prob, image)\n",
    "    \n",
    "    def generate_masks(self, image_paths: list[Path], output_dir: Path, skip_existing: bool = True) -> list[Path]:\n",
    "        \"\"\"Генерация масок для списка изображений.\"\"\"\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        n_rotations = 4 if self.cfg.use_rotations else 1\n",
    "        n_scales = len(self.cfg.scales)\n",
    "        n_flips = 3 if self.cfg.use_flips else 1  # hflip + vflip + original\n",
    "        n_tta = n_rotations * n_scales * n_flips\n",
    "        print(f\"TTA: {n_tta} passes ({n_rotations} rots × {n_scales} scales × {n_flips} flips) | Models: {len(self.models)}\")\n",
    "        \n",
    "        generated = []\n",
    "        for img_path in tqdm(image_paths, desc=\"Processing\"):\n",
    "            out_path = output_dir / img_path.name\n",
    "            if skip_existing and out_path.exists():\n",
    "                generated.append(out_path)\n",
    "                continue\n",
    "            \n",
    "            mask = self.process_image(img_path)\n",
    "            Image.fromarray(mask, mode=\"L\").save(out_path, format=\"TIFF\")\n",
    "            generated.append(out_path)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city(path: Path) -> str:\n",
    "    \"\"\"Извлечь город из имени файла.\"\"\"\n",
    "    return \"\".join(c for c in path.stem if not c.isdigit())\n",
    "\n",
    "\n",
    "def interleave_by_city(paths: list[Path]) -> list[Path]:\n",
    "    \"\"\"Равномерно перемешать по городам.\"\"\"\n",
    "    by_city: dict[str, list[Path]] = {}\n",
    "    for p in paths:\n",
    "        by_city.setdefault(get_city(p), []).append(p)\n",
    "    \n",
    "    for city in by_city:\n",
    "        by_city[city].sort()\n",
    "    \n",
    "    result, cities = [], sorted(by_city.keys())\n",
    "    indices = {c: 0 for c in cities}\n",
    "    \n",
    "    for _ in range(max(len(by_city[c]) for c in cities)):\n",
    "        for city in cities:\n",
    "            if indices[city] < len(by_city[city]):\n",
    "                result.append(by_city[city][indices[city]])\n",
    "                indices[city] += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_metrics(pred: np.ndarray, gt: np.ndarray) -> dict[str, float]:\n",
    "    \"\"\"Вычисление метрик сегментации.\"\"\"\n",
    "    p, g = pred > 0, gt > 0\n",
    "    intersection = np.logical_and(p, g).sum()\n",
    "    union = np.logical_or(p, g).sum()\n",
    "    tp = intersection\n",
    "    fp = np.logical_and(p, ~g).sum()\n",
    "    fn = np.logical_and(~p, g).sum()\n",
    "    tn = np.logical_and(~p, ~g).sum()\n",
    "    \n",
    "    iou = intersection / union if union > 0 else 1.0\n",
    "    dice = 2 * intersection / (p.sum() + g.sum()) if (p.sum() + g.sum()) > 0 else 1.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "    \n",
    "    return {\"iou\": iou, \"dice\": dice, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def run_validation(cfg: Config, generator: GroundTruthGenerator) -> pd.DataFrame:\n",
    "    \"\"\"Валидация на train данных.\"\"\"\n",
    "    all_images = list(cfg.train_images.glob(\"*.tif\"))\n",
    "    samples = interleave_by_city(all_images)[:cfg.debug_samples]\n",
    "    cities = sorted(set(get_city(p) for p in samples))\n",
    "    print(f\"Validating on {len(samples)} images from {len(cities)} cities\")\n",
    "    \n",
    "    generator.generate_masks(samples, cfg.val_gt, skip_existing=False)\n",
    "    \n",
    "    results = []\n",
    "    for p in samples:\n",
    "        pred = np.array(Image.open(cfg.val_gt / p.name))\n",
    "        gt = np.array(Image.open(cfg.train_gt / p.name))\n",
    "        results.append({\"filename\": p.name, **compute_metrics(pred, gt)})\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"VALIDATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df[[\"filename\", \"iou\", \"dice\", \"f1\", \"precision\", \"recall\", \"accuracy\"]].to_string(index=False, float_format=\"%.3f\"))\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Mean IoU:       {df['iou'].mean():.3f} ± {df['iou'].std():.3f}\")\n",
    "    print(f\"Mean Dice:      {df['dice'].mean():.3f} ± {df['dice'].std():.3f}\")\n",
    "    print(f\"Mean F1:        {df['f1'].mean():.3f} ± {df['f1'].std():.3f}\")\n",
    "    print(f\"Mean Precision: {df['precision'].mean():.3f} ± {df['precision'].std():.3f}\")\n",
    "    print(f\"Mean Recall:    {df['recall'].mean():.3f} ± {df['recall'].std():.3f}\")\n",
    "    print(f\"Mean Accuracy:  {df['accuracy'].mean():.3f} ± {df['accuracy'].std():.3f}\")\n",
    "    print(\"=\" * 70)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(cfg: Config, results_df: pd.DataFrame | None = None, n_samples: int = 3) -> None:\n",
    "    \"\"\"Визуализация результатов.\"\"\"\n",
    "    if results_df is not None:\n",
    "        sorted_df = results_df.sort_values(\"iou\")\n",
    "        samples = [sorted_df.iloc[i] for i in [0, len(sorted_df) // 2, len(sorted_df) - 1]]\n",
    "        labels = [\"WORST\", \"MEDIAN\", \"BEST\"]\n",
    "        images_dir, pred_dir, gt_dir = cfg.train_images, cfg.val_gt, cfg.train_gt\n",
    "    else:\n",
    "        all_masks = sorted(cfg.test_gt.glob(\"*.tif\"))\n",
    "        samples = random.sample(all_masks, min(n_samples, len(all_masks)))\n",
    "        labels = [m.name for m in samples]\n",
    "        images_dir, pred_dir, gt_dir = cfg.test_images, cfg.test_gt, None\n",
    "    \n",
    "    ncols = 4 if gt_dir else 3\n",
    "    fig, axes = plt.subplots(len(samples), ncols, figsize=(4 * ncols, 4 * len(samples)))\n",
    "    if len(samples) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, (sample, label) in enumerate(zip(samples, labels)):\n",
    "        filename = sample[\"filename\"] if isinstance(sample, pd.Series) else sample.name\n",
    "        iou = sample[\"iou\"] if isinstance(sample, pd.Series) else None\n",
    "        \n",
    "        image = np.array(Image.open(images_dir / filename))\n",
    "        pred = np.array(Image.open(pred_dir / filename))\n",
    "        \n",
    "        axes[i, 0].imshow(image)\n",
    "        axes[i, 0].set_title(f\"{label}: {filename}\" + (f\"\\nIoU={iou:.3f}\" if iou else \"\"))\n",
    "        axes[i, 0].axis(\"off\")\n",
    "        \n",
    "        axes[i, 1].imshow(pred, cmap=\"gray\")\n",
    "        axes[i, 1].set_title(\"Predicted\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "        \n",
    "        if gt_dir:\n",
    "            gt = np.array(Image.open(gt_dir / filename))\n",
    "            axes[i, 2].imshow(gt, cmap=\"gray\")\n",
    "            axes[i, 2].set_title(\"Ground Truth\")\n",
    "            axes[i, 2].axis(\"off\")\n",
    "            \n",
    "            # Error overlay: TP=green, FP=red, FN=blue\n",
    "            p, g = pred > 0, gt > 0\n",
    "            overlay = (image * 0.4).astype(np.uint8)\n",
    "            overlay[np.logical_and(p, g)] = [0, 200, 0]\n",
    "            overlay[np.logical_and(p, ~g)] = [200, 0, 0]\n",
    "            overlay[np.logical_and(~p, g)] = [0, 0, 200]\n",
    "            axes[i, 3].imshow(overlay)\n",
    "            axes[i, 3].set_title(\"Errors (G=TP, R=FP, B=FN)\")\n",
    "            axes[i, 3].axis(\"off\")\n",
    "        else:\n",
    "            overlay = image.copy()\n",
    "            overlay[pred > 0] = [255, 100, 100]\n",
    "            axes[i, 2].imshow(overlay)\n",
    "            axes[i, 2].set_title(\"Overlay\")\n",
    "            axes[i, 2].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|█| 1206/1206 [00:02<00:00, 428.43it/s, Materializing param\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Grounded SAM\n",
      "Loaded: YOLO Building\n",
      "Loaded 2 models\n"
     ]
    }
   ],
   "source": [
    "# Инициализация\n",
    "generator = GroundTruthGenerator(cfg)\n",
    "generator.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 180 test images\n",
      "TTA: 60 passes (4 rots × 5 scales × 3 flips) | Models: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  28%|█████▌              | 50/180 [102:51:17<267:25:22, 7405.56s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# Валидация\n",
    "if cfg.debug:\n",
    "    results_df = run_validation(cfg, generator)\n",
    "    visualize_results(cfg, results_df)\n",
    "else:\n",
    "    # Production: генерация на test\n",
    "    test_images = interleave_by_city(list(cfg.test_images.glob(\"*.tif\")))\n",
    "    print(f\"Found {len(test_images)} test images\")\n",
    "    generator.generate_masks(test_images, cfg.test_gt)\n",
    "    visualize_results(cfg, n_samples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dls-uv)",
   "language": "python",
   "name": "dls-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
